import requests
import pandas as pd
import re
import os
from requests.auth import HTTPBasicAuth

# Configuration
jenkins_base_url = "https://jenkins.crm.com"
username = "your_jenkins_username"  # Replace with your actual Jenkins username
password = "your_jenkins_password"  # Replace with your actual Jenkins password
output_excel = "jenkins_report.xlsx"

# List of SonarQube instances to check
sonar_instances = [
    r'https://sonar\.crm\.com[^\s]*',
    r'https://sonarenterprise\.crm\.com[^\s]*',
    r'https://crmsonar\.crm\.com[^\s]*'
]

# Function to save results to Excel
def save_to_excel(results, filename):
    df = pd.DataFrame(results)
    df.to_excel(filename, index=False)
    print(f"Excel updated: {filename}")

# Function to recursively fetch all jobs (including nested/tied jobs)
def fetch_all_jobs(url, auth, parent_path=""):
    jobs_list = []
    api_url = f"{url}/api/json?tree=jobs[name,url,jobs[name,url,jobs[name,url]]]"
    try:
        response = requests.get(api_url, auth=auth)
        if response.status_code != 200:
            print(f"Warning: Failed to fetch jobs from {url}: {response.status_code}")
            return jobs_list
        data = response.json()
        for job in data.get('jobs', []):
            name = job['name']
            job_url = job['url']
            full_name = f"{parent_path}{name}" if parent_path else name
            jobs_list.append({"name": full_name, "url": job_url})
            # Check for nested jobs (e.g., in folders or tied jobs)
            if 'jobs' in job and job['jobs']:
                nested_jobs = fetch_all_jobs(job_url, auth, f"{full_name}/")
                jobs_list.extend(nested_jobs)
    except Exception as e:
        print(f"Error fetching jobs from {url}: {e}")
    return jobs_list

# Step 1: Fetch all Jenkins jobs (including nested)
auth = HTTPBasicAuth(username, password)
jobs = fetch_all_jobs(jenkins_base_url, auth)

# Initialize results list
results = []

# Check if Excel file exists and load existing data
existing_jobs = set()
if os.path.exists(output_excel):
    try:
        existing_df = pd.read_excel(output_excel)
        results = existing_df.to_dict('records')  # Convert to list of dictionaries
        existing_jobs = set(existing_df['Job Name'].tolist())
        print(f"Found existing Excel with {len(existing_jobs)} jobs processed.")
        
        # Prompt user to choose whether to continue or start over
        choice = input("Do you want to (1) start from the beginning (overwrite Excel) or (2) continue where stopped? Enter 1 or 2: ").strip()
        if choice == "1":
            results = []  # Clear results to start fresh
            existing_jobs = set()
        elif choice != "2":
            print("Invalid choice. Defaulting to continue where stopped.")
    except Exception as e:
        print(f"Error reading existing Excel: {e}. Starting from the beginning.")
        results = []
        existing_jobs = set()
else:
    print("No existing Excel found. Starting from the beginning.")

# Process each job with progress indicator
total_jobs = len(jobs)
for index, job in enumerate(jobs, 1):
    name = job['name']
    
    # Skip jobs already processed if continuing
    if name in existing_jobs:
        print(f"Skipping job {index}/{total_jobs}: {name} (already processed)")
        continue
    
    print(f"Processing job {index}/{total_jobs}: {name}")
    url = job['url']
    job_result = {
        "Job Name": name,
        "Url": url,
        "Sonarqube": "not found",
        "Dependency Track": "No",
        "Bitbucket": "not found"
    }

    # Step 2-4: Fetch console text for the last successful build on develop branch
    console_url = f"{url}job/develop/lastSuccessfulBuild/consoleText"
    console_response = requests.get(console_url, auth=auth)

    if console_response.status_code != 200:
        print(f"Warning: Failed to fetch console text for {name}: {console_response.status_code}")
        results.append(job_result)
        save_to_excel(results, output_excel)  # Save progress
        continue

    console_text = console_response.text

    # Check for Dependency Track
    if "The artifact was successfully published" in console_text:
        job_result["Dependency Track"] = "Yes"

    # Parse lines for Sonarqube and Bitbucket
    lines = console_text.split('\n')
    for line in lines:
        # Step 2: Check for Sonarqube (across all instances)
        if "ANALYSIS SUCCESSFUL" in line and job_result["Sonarqube"] == "not found":
            for sonar_pattern in sonar_instances:
                sonar_match = re.search(sonar_pattern, line)
                if sonar_match:
                    job_result["Sonarqube"] = sonar_match.group(0)
                    break  # Use the first matching SonarQube URL

        # Step 4: Check for Bitbucket
        if "Cloning repository" in line and job_result["Bitbucket"] == "not found":
            bitbucket_match = re.search(r'Cloning repository (https://bitbucket\.crm\.com[^\s]*)', line)
            if bitbucket_match:
                job_result["Bitbucket"] = bitbucket_match.group(1)

    results.append(job_result)
    save_to_excel(results, output_excel)  # Save progress after each job

print(f"\nReport generation complete: {output_excel}")